{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fffa267",
   "metadata": {},
   "source": [
    "## Comparing a Neural Network and Logistic Regression Approach to Predicting Game Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe4aade",
   "metadata": {},
   "source": [
    "The idea below is simple. Take two variables: (1) the home team's adjusted net margin for the season and (2) the away team's adjusted net margin to predict the proabbility the home team will win. \n",
    "\n",
    "The main variables are the average margin of victory over the season, adjusted for strengths of opponents. This is not as straightforward as you think: early in the season there is no game data (eg Game 1) and for those early games I used an empirical bayes approach to create a best guess for each team's prior performance, and slowly update as the current season gets more data. By Game 20, only the current season's data is used (sort of a smooth updating process).\n",
    "\n",
    "Then, I manually constructed a neural network with one layer that tries to best classify victories, and compare its performance to a simple logistic regression. ```NBAData_NeuralNet.R``` is quite thorough and well-commented, so hopefully every step is clear: I go through each step of the calculation (as custom functions) from instantiating parameters, to forward propogation, gradient computation, backpropogation to update weights, and iterations until there is some convergence or a certain number of iterations is reach. Then I run code to compare the out-of-sample and in-sample performance of each model.\n",
    "\n",
    "I run the entire process below. Hopefully the outputs are clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836d966a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.6     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.4\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.7     \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.9\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.0     \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0\n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.2     \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.1\n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Training Neur Net Model \n",
      "\n",
      "Iteration 100  | loss:  0.6108617  | converge:  4.400048e-06 \n",
      "Iteration 200  | loss:  0.6106936  | converge:  7.404783e-07 \n",
      "Iteration 300  | loss:  0.6109665  | converge:  0.0001013735 \n",
      "Iteration 400  | loss:  0.611292  | converge:  6.627319e-05 \n",
      "Iteration 500  | loss:  0.6112374  | converge:  7.883351e-06 \n",
      "Iteration 600  | loss:  0.6112386  | converge:  4.528652e-06 \n",
      "Iteration 700  | loss:  0.6112398  | converge:  2.515468e-06 \n",
      "Iteration 800  | loss:  0.611237  | converge:  2.99666e-06 \n",
      "Iteration 900  | loss:  0.611236  | converge:  3.034028e-06 \n",
      "Iteration 1000  | loss:  0.6112348  | converge:  3.102378e-06 \n",
      "Iteration finished at  1000  | loss:  0.6112348  no converge \n",
      "\n",
      "STEP 2a: Confusion Matrix for NN (out of sample test) \n",
      "\n",
      "      y_predclass\n",
      "y_test    0    1\n",
      "     0   85  808\n",
      "     1   17 1268\n",
      "\n",
      "\n",
      "\n",
      "STEP 2b: Confusion Matrix for Logistic Reg (out of sample test) \n",
      "\n",
      "      logistic_predclass\n",
      "y_test    0    1\n",
      "     0  417  476\n",
      "     1  267 1018\n",
      "\n",
      "\n",
      "STEP 3: Calculate out-of-sample tests and compare results on many factors\n",
      "\n",
      "NN  (out of sample test): \n",
      "\tAccuracy =  62.12121 %.\n",
      "\tPrecision =  98.67704 %.\n",
      "\tRecall =  61.079 %.\n",
      "\tF1 Score =  75.45373 %.\n",
      "\tBin. Cross. Ent.  =  0.6448013 \n",
      "\n",
      "logreg  (out of sample test): \n",
      "\tAccuracy =  65.88613 %.\n",
      "\tPrecision =  79.22179 %.\n",
      "\tRecall =  68.13922 %.\n",
      "\tF1 Score =  73.26376 %.\n",
      "\tBin. Cross. Ent.  =  0.6202111 \n",
      "\n",
      "STEP 4a: Confusion Matrix for NN (in-sample test) \n",
      "\n",
      "       y_predtrainclass\n",
      "y_train    0    1\n",
      "      0 4015 4100\n",
      "      1 2470 9022\n",
      "\n",
      "\n",
      "\n",
      "STEP 4b: Confusion Matrix for Logistic Reg (in-sample test) \n",
      "\n",
      "       logistic_predtrainclass\n",
      "y_train    0    1\n",
      "      0 3745 4370\n",
      "      1 2217 9275\n",
      "\n",
      "\n",
      "STEP 5: Calculate out-of-sample tests and compare results on many factors\n",
      "\n",
      "Neur. Net. : in sample test \n",
      "\tAccuracy =  66.49156 %.\n",
      "\tPrecision =  78.50679 %.\n",
      "\tRecall =  68.75476 %.\n",
      "\tF1 Score =  73.30787 %.\n",
      "\tBin. Cross. Ent.  =  0.6112379 \n",
      "\n",
      "logreg : in sample test \n",
      "\tAccuracy =  66.40486 %.\n",
      "\tPrecision =  80.70832 %.\n",
      "\tRecall =  67.97362 %.\n",
      "\tF1 Score =  73.7956 %.\n",
      "\tBin. Cross. Ent.  =  0.6163194 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#INPUT: folder where git repository lives on your computer\n",
    "#setwd(\"/Users/To/GitHub/R/\")\n",
    "#setwd(\"/Users/To/GitHub/R/\")\n",
    "setwd(\"/Users/ricardovelloso/git-projects/NBA_Data_Project/R/\")\n",
    "suppressWarnings({source(\"NBAData_NeuralNet.R\")}) #gets data from public dropbox link with my data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281de825",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "This shows something I like to call the \"kind-of law of small/medium samples\". That is, when you have a strong intuition for the data, and what features will be important (in my view, a logistic regression linear in the average net margins is a pretty close approximation to the \"true\" data process), and the sample is not very big (~20,000 rows), then simple regressions tend to perform really well, and more advanced techniques can often have a lot of problems. And, of course, you get the interpretibility/efficiency of linear regression without all the opaqueness of more advanced models.\n",
    "\n",
    "Of course, it is a matter of execution. In my example, I intentionally created a simple neural network (1 layer with 4 neurons) to show how quickly/easily these sorts of models over-fit the training data. They perform much better in-sample, but much worse out-of-sample. In this case I could literally replicate the logistic regression process in some form in the neural network, but have chosen not to. You have to be very careful about selecting the number of weighting parameters and activation functions used, as well as the loss function (in this case regularization would go a long way).\n",
    "\n",
    "Steps 2 and 3 compare my simple neural network to the logistic regression. The binary cross entropy on the out-of-sample test is much lower for logistic regression (.623 vs .647 for the neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e29cd",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "I have done much more complicated versions of each (many more variables) and the difficulty of using the NN scales with the number and complexity of the features you choose to include, but, of course, both produce fairly accurate results once you have cross-validated successfully. You should view this as a \"proof of concept\" that I know what underlies the neural network process. Any more complicated code scales in a similar manner, and of course I usually tend to utilize pre-existing packages in R and Python if the network I am fitting is relatively straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99af249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
